---
title: What is theoretical computer science?
date: 2019-12-23 00:05 -0500
tags: [tcs, complexity-theory]
---

This past fall, I was fortunate to be member of the course staff for [CS 121: Introduction to Theoretical Computer Science](https://cs121.boazbarak.org) at Harvard. The field is completely fascinating to me, and I want to pursue it further, but I have had a lot of trouble articulating types of things that theoretical computer scientists think or care about outside of class, and in particular to my family! The goal, therefore, of this post is to be a *very* informal introduction to the subject. I also hope you take it with a grain of salt, since my perspective is certainly bounded by my exposure to the material (and perhaps also by my youth ðŸ˜Š)!

At a high level, the goal of *theoretical computer science (TCS)* is to approach computation from a mathematical perspective and seek to understand the mathematical limits of computational ability. In other words, the fundamental questions of TCS are about whether a computer can carry out a certain type of mathematical calculation or achieve a certain mathematical goal. A sampling of the countless questions theoretical computer scientists think about includes:

* Are there any problems that can't be solved by computers at all?
* We know that computers can factor numbers --- a *brute force* method is to just check all possible divisors --- but is it possible to factor a number *efficiently*?
* How can information be transmitted reliably over error-prone channels?
* Can quantum computers efficiently solve any problems that normal computers can't solve efficiently?
* Why do artificial intelligence models perform unexpectedly well on, say, image recognition tasks? Can we prove that, under certain conditions, they should perform well?
* How can we be sure that a computer program functions according to its specification? (Say, we are the Air Force and we want to make sure that a fighter jet's central computer won't malfunction.)
* What does it mean for an encryption scheme to be secure? How can we prove that one is secure?
* Can we know for sure that an algorithm protects user privacy? Can we know for sure that an algorithm is fair and unbiased?

TCS attempts to tackle these questions via *mathematical proof*. That is, the process when answering each of the above questions is to first define the terms mathematically; make reasonable conjectures about them; and then prove the conjectures from first principles.

In what follows, I'll try and sample some classic notions and questions of theoretical computer science, and at the end, I also wrap it up by explaining a bit about why I find these questions so interesting.

### Computational complexity theory

In very broad strokes, a computer is just a glorified calculator; it can perform various kinds of arithmetic operations and store the results in internal storage (e.g. *random access memory (RAM)*). In fact, when a CPU (the actual computational processor of the computer) is described as running at, say, 2GHz, that means that it can perform *two billions calculations per second*; this is a relatively reasonable speed among modern computers. Sophisticated supercomputers that are custom-designed for quickly performing calculations can surpass this speed dramatically to try and run more computationally-intensive calculations.

Overall, the goal of *computational complexity theory* is to understand what mathematical problems computers can solve, and the amount of resources required to solve them. For instance, there are some problems that we believe that no reasonably powerful computer could solve in one hundred million years; other problems, we know that modern computers can solve in nanoseconds. Complexity theorists seek to understand and classify computational problems according to their "hardness of solving". (Note that these problems are well-defined, mathematically-specifiable questions, not broad questions like "Can a computer understand emotions?" that cannot easily be stated on a mathematical footing.)

Computational complexity is probably the most well-studied subfield of theoretical computer science. Why do people care so much about it? Here are a few motivations:

1. Many of the most well-studied problems have extremely important applications in industry and research. One important class of problems is *optimization problems*, which seek to find the "best solution" out of a set of possible solutions; these tend to arise directly from important problems in applications. For instance, the *traveling salesman problem* (find the shortest route for a truck to visit every town on a list of towns exactly once) is essential in routing and distribution systems. Finding good algorithms for such problems would likely lead to both economic advances and advances in research (e.g., in medicine).
2. Understanding the *hardness* of computational problems also has important applications in fields such as *cryptography*. Many "ciphers" are based off *trapdoors*, which make a certain problem (like decrypting encrypted data) only possible when you have some "secret knowledge". For instance, the *RSA cryptosystem*, which is used to protect virtually all traffic on the Internet, is based off of the idea that *integer factoring* is a hard problem --- Alice chooses two large primes $$p$$ and $$q$$ and publicly releases $$N = p \cdot q$$; Bob can send messages to Alice encrypted using the publicly known $$N$$, but in order to decrypt the messages, one obtain must the factorization of $$N$$ into $$p$$ and $$q$$, and only Alice knows this by design. Thus, being able to show that integer factoring is computationally hard would go a great way towards helping shore up the foundations of cryptography. (RSA actually relies on some stronger assumptions than just integer factoring being hard, but if integer factoring were easy, RSA would not be secure.)
3. The theory of complexity is mathematically beautiful and incorporates ideas from almost all areas of modern mathematics; it is also a relatively new field compared to traditional branches of mathematics, so there is (hopefully) lots of progress still to be made.

Note that (1) is a reason why showing problems are easy to solve is important and (2) is a reason why showing problems are hard to solve is important, so regardless of outcome, being able to definitively answer complexity questions is a win-win.

### Mathematical ingredients

The main mathematical ingredients necessary for understanding complexity are the following:

1. *Computational problems*. These are mathematical questions that, in the simplest case, just have yes/no answers. For instance, "Given $$x$$, $$y$$, and $$z$$, does $$x+y=z$$?" is a problem that any modern computer can solve easily. "Given $$x$$ and $$a$$, does $$x$$ have a factor that is at least $$a$$?" is a problem that is believed to be difficult to solve.
2. *Algorithms* (a.k.a. *programs*). These are computational processes that transform an *input* to an *output* based on a specific list of instructions.
3. *Running time*. Given a particular algorithm, running on a particular input, how many steps will the algorithms take before finishing its computation and returning the result? 

Complexity theorists think about complexity in terms of *asymptotics*; that is, they want to understand, "As the inputs to an algorithm get more complex, how does the running time of the algorithm scale?". For example, consider the problem of checking whether a list of $$n$$ elements is in sorted order. A simple algorithm for this problem is to iteratively verify that each adjacent pair of elements in the list is in order. We say this algorithm has *linear* runtime: On inputs of size $$n$$, the amount of steps it requires scales directly with $$n$$ (and not, e.g., $$n^2$$).

Note that the asymptotic scaling of an algorithm's runtime is a huge deal. If the algorithm runs in time proportional to $$n$$, then would take, say, $$1000$$ steps to solve an input of length $$n$$; an $$n^2$$-time algorithm would require a million steps, while a $$2^n$$-time algorithm would take more steps than we believe there to be particles in the universe.

### Major questions in complexity

The central goal of complexity theory is *comparative*; we wish to understand how the problems we can solve change if we allow a computer access to more resources. Thus, it's important to define other notions of resources as "measuring sticks" that lead to comparative questions for complexity theorists to think about; each of the following has spawned hundreds of papers and decades of research, and this list is by no means exhaustive:

- *Nondeterminism* is, at a high level, the ability to search an entire set of things simultaneously. *The* central question of computer science is whether nondeterminism lets you solve more problems. In other words, are there any problems for which a solution can be quickly verified, but not quickly found? This problem is known as the $$\mathbf{P}$$ vs. $$\mathbf{NP}$$ problem and it is one of the top open problems in all of mathematics. I am planning to write a nontechnical exposition of the problem in a follow-up blog post to help give more "flavor" of the types of things complexity theorists think about.
- *Randomized algorithms* are algorithms that are allowed to be wrong *sometimes*, but must be right a most of the time. The central question: Does allowing randomness let us solve more problems efficiently? There is also a notion of *approximation algorithms*, which must get a close-to-correct answer for all instances of a given problem (as opposed to the correct answer on most instances). *Quantum algorithms* are algorithms that, very roughly, utilize a complicated "physics" form of randomness in which probabilities are allowed to be complex numbers (i.e. can "include" $$i = \sqrt{-1}$$) instead of traditional numbers between zero and one. This allows the outcomes to be added interfere with each other in unintuitive ways, and the goal of a quantum algorithm is to cause the outcomes to interfere in just the right way, so that the probability of measuring the desired outcome is maximized. The central question: Does allowing quantum randomness let us solve more problems efficiently?
- *Communication complexity* seeks to quantify the amount of information two parties must exchange in order to achieve a common computational goal. This is often nice to study because the goal is to quantify communication, not computational quantities such as running time. Models and results based on multiple communicating parties also involve notions such as *error-correcting codes*, which can be used to recover an original message if it is sent across a channel that adds corruptions, and *interactive proofs*, where one or more powerful provers must convince a less-powerful verifier that something is true. Communication and randomness together form the basis for the theoretical analysis of cryptography.
- *Circuits* are computational processes that require a fixed input length instead of a variable input length; for instance, a circuit might add 10-digit numbers, instead of adding arbitrary numbers. Circuits are built from logical operations on ones and zeros. Theorists like thinking about circuits because they seem simpler than algorithms (e.g., it is possible to enumerate over all possible circuits and see if any small circuit solves a task). However, many problems in circuit complexity have turned out to be extremely hard.

### Personal note

Aside from the oftentimes surprising mathematical results and the number of exciting open problems, there are a number of things that make TCS and complexity theory in particular really fascinating for me. As someone whose background is mostly on the programming side, I think a lot of the things that TCS considers make sense --- e.g., "Can I do this efficiently or not? If not, can I get close?" are the type of questions that you ask yourself constantly during programming. The natural evolution of such questions is complexity theory. And as compared to the rest of math, I appreciate TCS a lot, because it's more computational and the proof styles feel more grounded. We actually care about finding efficient algorithms, not just proving that they exist! Many of the proofs are constructive in the sense that you provide an algorithm for accomplishing what you want to show is possible. You also have to be more careful with accounting for all of the costs of what you're doing; whereas oftentimes in, say, analysis, you only care that some error goes to zero in some limit, here, we care about how quickly the error goes to zero.

I am very excited to continue exploring all of these questions in the coming months and years. In addition, I hope to use this post as a springboard to start writing more regularly about the types of things I'm up to and things that are interesting to me.