---
title: P and NP
date: 2019-10-08 -0500
tags: [tcs, complexity-theory]
---

I want to continue my brief tour of complexity theory by describing the $$\mathbf{P}$$ vs. $$\mathbf{NP}$$ problem. This problem is fascinating on its own, and it also provides a more concrete "flavor" of the types of things complexity theorists think about.

The *$$\mathbf{P}$$ vs. $$\mathbf{NP}$$ problem* is the most famous open problem in complexity theory, and one of the most famous open problems in all of mathematics. $$\mathbf{P}$$ and $$\mathbf{NP}$$ both stand for an *entire class* of computational problems (such classes are called *complexity classes*). If a computational problem is in the class $$\mathbf{P}$$, that means that it can be solved efficiently; if a computational problem is in the class $$\mathbf{NP}$$, that means that a solution can be verified efficiently.

Intuitively, we can think of problems in $$\mathbf{NP}$$ as problems that require checking a solution, while problems in $$\mathbf{P}$$ correspond to finding a solution. Every problem in $$\mathbf{P}$$ is also guaranteed to be in $$\mathbf{NP}$$; the fundamental question is whether there are problems in $$\mathbf{NP}$$ that are not in $$\mathbf{P}$$ (mathematicians call this being a *strict subset* of $$\mathbf{NP}$$). The vast majority of the theoretical computer scientists conjecture that this is indeed the case; however, despite the best efforts of thousands of researchers, a proof of this fact has remained deeply elusive.

Why is such a statement so difficult to prove? Well, it requires being able to establish *lower bounds* on the complexity of certain computational problems. That is to say, we must prove statements of the form, "There is no algorithm that solves this problem efficiently". Since there are infinitely many possible algorithms, this goal is extremely difficult to achieve. (Note, correspondingly, that establishing *upper bounds* on complexity is often much simpler. It only requires actually giving an algorithm to solve a problem.) In fact, strong lower bounds are in many ways the holy grail for theoretical computer scientists; yet it is not known how to prove even conjectured lower bounds that much weaker than what would be required to show $$\mathbf{P} \neq \mathbf{NP}$$.

One crucial property that helps us better understand the $$\mathbf{P}$$ vs. $$\mathbf{NP}$$ problem is the existence of so-called *$$\mathbf{NP}$$-complete* problems, which are, in a certain sense, the "hardest problems in $$\mathbf{NP}$$". In particular, giving a strong lower bound for any $$\mathbf{NP}$$-complete problem would suffice to show that $$\mathbf{P} \neq \mathbf{NP}$$, while conversely, any efficient algorithm for any $$\mathbf{NP}$$-complete problem would immediately imply that $$\mathbf{P} = \mathbf{NP}$$. And it turns out that these problems, aside from being of theoretical interest, often have immense practical applications in industry; being able to either find efficient algorithms for them, or understand why they lack efficient algorithms, would therefore have huge real-world ramifications.

There are literally thousands of known $$\mathbf{NP}$$-complete problems; I will briefly mention three classic ones with somewhat different "flavors" that might help build some intuition:

- The most famous $$\mathbf{NP}$$-complete problem, a variant of which was also the first to be proven $$\mathbf{NP}$$-complete, is the *Boolean satisfiability* problem. A *Boolean formula* is a statement built out of logical ANDs and ORs from some basic elements $$x_1, x_2, \ldots, x_n$$ being either true or false. For instance, "($$x_1$$ is true or $$x_3$$ is false) and ($$x_2$$ is true or $$x_3$$ is true)" is a Boolean formula. This particular formula is actually *satisfiable*, since we could for example say that $$x_1$$, $$x_2$$, and $$x_3$$ are all true; this makes the entire statement true. On the other hand, a formula such as "$$x_1$$ is true and $$x_1$$ is false" is never satisfiable. The Boolean satisfiability problem asks, in general, whether a given formula is satisfiable.
- The *subset sum* problem is to, given a set of numbers $$\{a_1, \ldots, a_n\}$$, determine if there is some (nonempty) subset that sums to zero.
- The *maximum cut* problem is to, given a network of houses connected by pipes of different capacities, determine if there is some way to divide the houses into two groups such that the total capacity of the pipes that pass between the groups exceeds some number $$k$$.

Note the crucial property that all of these problems have in common: Solutions to them can be easily checked, while there is no obvious way to actually find a solution aside from the inefficient brute force method.


### What do complexity theorists spend their time doing?

There are literally thousands of known complexity classes, and if you ask a random computer scientist if to given classes are different, chances are, they'll say "I don't know, but we conjecture that they are". However, most theoretical computer scientists also believe that we're a long way off from being able to actually prove results such as $$\mathbf{P} \neq \mathbf{N}$$. So what do theoretical computer scientists do with their time?

Firstly, they prove implications among conjectures; indeed, many of the most beautiful results in theoretical computer science have only been proven true conditioned on the truth of certain widely-believed conjectures such as $$\mathbf{P} \neq \mathbf{N}$$.

Secondly, practitioners of the field send a lot of time thinking about what techniques might actually, someday, lead to a proof that $$\mathbf{P} \neq \mathbf{N}$$. Unfortunately, it turns out that many of the most widely used proof techniques in theoretical computer science are provably “too weak” to establish such separations. The effect is that there is a dazzlingly rich world of conjectures, many of which are completely unknown how to prove, yet there is a beautiful structure of implications and conditional proofs among them. So when results are actually achieved in certain areas, they often immediately carry over to help us understand many other open problems.

### Why should we believe anything complexity theorists say?

Why do theoretical computer scientists have confidence in these results since we haven’t been able to prove them? A common criticism of the field is that in many ways it is a house of cards, which might all come crashing down if a small number of counterexamples to widely accepted conjectures were discovered. The two main responses are: 1) The world that we live in is so mathematically intricate and natural that, on some Platonic level, it cannot be the case that our assumptions are wrong, and 2) Empirically, if, for instance, 3SAT did have an efficient algorithm, the brilliant community of algorithmists should certainly have seen some indication of what such an algorithm might look like after decades of concerted effort. The skeptics respond that theoretical computer scientists have also been on able to discover a proof that 3SAT has no efficient algorithm; however, we are beginning to see what the parameters of such a proof of have to look like, and, in general, it isn’t many times more difficult to prove lower bounds and upper bounds (see scott).